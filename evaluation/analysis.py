# -*- coding: utf-8 -*-
"""analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NtpKO1p9sYcvge1C-D2emm2qXhZj_nil
"""

import cv2
import numpy as np
# from ultralytics import YOLO

cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\2025-02-26_14-38-06.mp4")
timestamps = [cap.get(cv2.CAP_PROP_POS_MSEC)]

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

filter_out = cv2.VideoWriter(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_filter.mp4", cv2.VideoWriter_fourcc(*"mp4v"), 15, (width, height))

prev_frame_grey = None
wakeup_time_left = 0

while cap.isOpened():
    ret, frame =  cap.read()
    if not ret:
        break


    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    if prev_frame_grey is None:
        prev_frame_grey = frame_grey

    frame_difference = cv2.absdiff(prev_frame_grey, frame_grey)

    _, filter_frame = cv2.threshold(frame_difference, 30, 255, cv2.THRESH_BINARY)

    cv2.imshow("Filtered", filter_frame)

    pixels_changed = np.sum(filter_frame)/255

    # print(timestamps)

    if pixels_changed > 5000:
        filter_out.write(filter_frame)
        timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC))

    prev_frame_grey = frame_grey

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

filter_frame.release()

"""# Filter Analysis"""

import cv2
import numpy as np
# from ultralytics import YOLO

raw_cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_raw.mp4")
filter_cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_filter_raw.mp4")

timestamps = [raw_cap.get(cv2.CAP_PROP_POS_MSEC)]

width = int(raw_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(raw_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

prev_frame_grey = None
wakeup_time_left = 0

frame = 0
while raw_cap.isOpened():
    # print(frame)
    frame += 1
    ret, raw_frame =  raw_cap.read()
    _, filter_frame = filter_cap.read()

    if not ret:
        break

    filter_frame = cv2.cvtColor(filter_frame, cv2.COLOR_BGR2GRAY)
    pixels = (filter_frame>30).sum()

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

"""# Threshold Histogram"""

import cv2
import numpy as np
# from ultralytics import YOLO

cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\2025-02-26_14-38-06.mp4")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# filter_out = cv2.VideoWriter(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_filter.mp4", cv2.VideoWriter_fourcc(*"mp4v"), 15, (width, height))
# filter_raw = cv2.VideoWriter(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_filter_raw.mp4", cv2.VideoWriter_fourcc(*"mp4v"), 15, (width, height))
# raw_out = cv2.VideoWriter(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_raw.mp4", cv2.VideoWriter_fourcc(*"mp4v"), 15, (width, height))

key_frames = [1, 17, 99, 258, 277, 331, 333, 361, 383, 444, 461, 538, 577, 595, 618, 647, 704, 726, 742, 768, 818, 844, 878, 892, 910, 1018, 1104, 1127, 1186, 1205, 1224, 1289, 1334, 1358, 1379, 1394, 1406, 1433, 1446, 1484,
 1492, 1533, 1563, 1677, 1690, 1717, 1741, 1776, 1859, 2700, 2791, 2823, 2836, 2866, 2944, 2998, 3033, 3054, 3085, 3172, 3179, 3308, 3342, 3439, 3599, 3623, 3638, 3690, 3696, 3817, 3837, 3915, 3925, 3956, 3976,
 4098, 4135, 4170, 4192, 4213, 4246, 4272, 4301, 4315, 4320, 4340, 4362, 4389, 4394, 4444, 4464, 4483, 4484]
END = 4626
key_frames_raw = []

prev_frame_grey = None
wakeup_time_left = 0

frame_num = 1
raw_frame_num = 0
is_correct = False
while cap.isOpened():
    ret, frame =  cap.read()
    if not ret:
        break

    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    if prev_frame_grey is None:
        prev_frame_grey = frame_grey

    frame_difference = cv2.absdiff(prev_frame_grey, frame_grey)

    _, filter_frame = cv2.threshold(frame_difference, 30, 255, cv2.THRESH_BINARY)

    pixels_changed = np.sum(filter_frame)/255

    # print(timestamps)

    if pixels_changed > 5000:
        # filter_frame = cv2.cvtColor(filter_frame, cv2.COLOR_GRAY2BGR)
        # filter_raw.write(filter_frame)

        # cv2.putText(filter_frame, f"pixels: {int(pixels_changed)}", (0,25), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 3)
        # cv2.putText(filter_frame, f"{raw_frame_num}", (width-100,25), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 3)
        # cv2.imshow("Filtered", filter_frame)

        # cv2.putText(frame, f"{frame_num}", (width-100,25), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 3)

        if frame_num in key_frames:
            is_correct = not is_correct
            key_frames_raw.append(raw_frame_num)
        if frame_num == END:
            break
        frame_num += 1
    else:
        if is_correct:
            key_frames_raw.append(raw_frame_num)
            is_correct = not is_correct
        # raw_out.write(frame)
        # filter_out.write(filter_frame)
        # timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC))

    prev_frame_grey = frame_grey

    raw_frame_num += 1

    # if cv2.waitKey(1) & 0xFF == ord('q'):
    #     break

# with open("timestamps.txt", "w") as f:
#     print(len(timestamps))
#     f.write(str(timestamps))

cap.release()
# raw_out.release()
# filter_out.release()
# filter_raw.release()

# # END = 81909
# # key_frames_raw = key_frames_raw - 1
# np.save(r"data\key_frames_raw_02_26.npy", key_frames_raw)
key_frames_raw = [190, 204, 3792, 3814, 4082, 4107, 5072, 5732, 6134, 6151, 6315, 6335, 6731, 6761, 6923, 6949, 7371, 7395, 8363, 8378, 8463, 8480, 8729, 8751, 8763, 8786, 9606, 9636, 11572, 11589, 12262, 12290, 16232, 16252,
                  16789, 16814, 19273, 19295, 20324, 20336, 24327, 24652, 26378, 26400, 27571, 27589, 28885, 28915, 29521, 29577, 32584, 32606, 33302, 33316, 34087, 34112, 34538, 34574, 35277, 35304, 35812, 35822, 37139, 37150,
                  37375, 37394, 37438, 37471, 38055, 38099, 38893, 38919, 39676, 39709, 42466, 45304, 51931, 51962, 52572, 52602, 55201, 55220, 55314, 55347, 56192, 56211, 56393, 56421, 58022, 58047, 58574, 58602, 59606, 60252,
                  63461, 63485, 63822, 63848, 64165, 64954, 64982, 69540, 69562, 70092, 70118, 71213, 71235, 71454, 71480, 72003, 72033, 72137, 72166, 72236, 72250, 72310, 72325, 73551, 73565, 73802, 74207, 74516, 74545, 75168,
                  75698, 75943, 76005, 77325, 77345, 78097, 78122, 78930, 78943, 79166, 79187, 79793, 79805, 80257, 80270, 80483, 80511, 81038, 81056, 81847, 81864, 81908, 81951]

import cv2
import numpy as np

num_of_pixels_changed = []
cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\2025-02-26_14-38-06.mp4")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))


prev_frame_grey = None
raw_frame_num = 0

# threshold_data = []

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    if prev_frame_grey is None:
        prev_frame_grey = frame_grey

    frame_difference = cv2.absdiff(prev_frame_grey, frame_grey)

    prev_frame_grey = frame_grey

    _, filter_frame = cv2.threshold(frame_difference, 30, 255, cv2.THRESH_BINARY)

    pixels_changed = np.sum(filter_frame)/255

    num_of_pixels_changed.append(pixels_changed)

    if raw_frame_num == key_frames_raw[-1]:
        break

    raw_frame_num += 1

cap.release()
threshold_data.append(num_of_pixels_changed)

cap.release()

import cv2
import numpy as np
from tqdm import tqdm
# from ultralytics import YOLO

pixel_value_thresholds = np.linspace(0, 100, 11)

threshold_data = []

for pixel_threshold in tqdm(pixel_value_thresholds):
    num_of_pixels_changed = []
    cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\2025-02-26_14-38-06.mp4")

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))


    prev_frame_grey = None
    raw_frame_num = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if prev_frame_grey is None:
            prev_frame_grey = frame_grey

        frame_difference = cv2.absdiff(prev_frame_grey, frame_grey)

        prev_frame_grey = frame_grey

        _, filter_frame = cv2.threshold(frame_difference, pixel_threshold, 255, cv2.THRESH_BINARY)

        pixels_changed = np.sum(filter_frame)/255

        num_of_pixels_changed.append(pixels_changed)

        if raw_frame_num == key_frames_raw[-1]:
            break

        raw_frame_num += 1

    cap.release()
    threshold_data.append(num_of_pixels_changed)

    cap.release()

import numpy as np
# np.save(r"data\threshold_data_new.npy", threshold_data)
# pixel_value_thresholds
threshold_data = np.load(r"data\threshold_data.npy")
threshold_data
# np.linspace(0, 250, 26)

is_correct_key_frames = []
print(len(key_frames_raw))
for i in range(0, 146, 2):
    start = key_frames_raw[i]
    stop = key_frames_raw[i+1]
    is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))
is_correct_key_frames

pixel_count_thresholds = np.linspace(0, 10000, 101)
key_frames_raw = np.array(key_frames_raw)
precision_scores = np.zeros((11, 101))
recall_scores = np.zeros((11, 101))
f2_scores = np.zeros((11, 101))

for i, pt in enumerate(pixel_value_thresholds):
    cap_data = np.array(threshold_data[i])

    for j, pixel_count_thresh in enumerate(pixel_count_thresholds):
        print(f"Pixel threshold: {pt}, Pixel Count Threshold: {pixel_count_thresh}")
        cap_data_selected = cap_data > pixel_count_thresh
        TP = np.sum(cap_data_selected[is_correct_key_frames])
        FP = np.sum(cap_data_selected) - TP
        FN = np.sum((~cap_data_selected)[is_correct_key_frames])
        precision = TP/(TP+FP)
        recall = TP/(TP+FN)

        print(f"Precision: {precision}, Recall: {recall}")

        # F2 = 5/((4/recall)+1/precision)
        F1 = 2*(recall*precision)/(precision+recall)

        precision_scores[i, j] = precision
        recall_scores[i, j] = recall
        f2_scores[i, j] = F1

        print(f"F2 Score: {F1}")


# f1_scores = np.zeros((10, ))

# f1_scores[np.isnan(f1_scores)] = 0
pixel_count_thresholds

import matplotlib.pyplot as plt

# plt.figure(figsize=(10, 8))
plt.imshow(f2_scores, interpolation='nearest', cmap='viridis', aspect="auto")
plt.title('F2 score Heatmap')
plt.xlabel('Pixel Count Threshold')
plt.ylabel('Pixel Difference Threshold')
plt.colorbar(label='F2 score')
plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
plt.yticks(ticks=np.arange(len(pixel_value_thresholds)), labels=np.array(pixel_value_thresholds, dtype=np.int64))
plt.show()

np.argmax(f2_scores)
# f2_scores[2, 94]
# pixel_count_thresholds[94]

"""# Analysis of the pixel thresholds wrt to time"""

import numpy as np
# np.save(r"data\threshold_data.npy", threshold_data)
# pixel_value_thresholds
threshold_data = np.load(r"data\threshold_data.npy")
key_frames_raw = np.load(r"data\key_frames_raw_02_26.npy")

is_correct_key_frames = []
for i in range(0, 182, 2):
    start = key_frames_raw[i]
    stop = key_frames_raw[i+1]
    is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))
print(is_correct_key_frames[-1])
threshold_data.shape
# np.linspace(0, 250, 26)

# There is 192min of footage from the captured images from 2:38pm to 5:50pm

pixel_value_thresholds = np.linspace(0, 100, 11)
pixel_count_thresholds = np.linspace(0, 10000, 101)
key_frames_raw = np.array(key_frames_raw)
precision_scores = np.zeros((11, 101, 100))
recall_scores = np.zeros((11, 101, 100))
f2_scores = np.zeros((11, 101, 100))
events = np.zeros((11, 101, 100))
buckets = np.linspace(0, 81952, 101, dtype=np.int32)
for i, pt in enumerate(pixel_value_thresholds):
    # if pt != 20:
    #     continue
    cap_data = np.array(threshold_data[i])

    for j, pixel_count_thresh in enumerate(pixel_count_thresholds):
        # if pixel_count_thresh != 9400:
        #     continue
        print(f"Pixel threshold: {pt}, Pixel Count Threshold: {pixel_count_thresh}")
        cap_data_selected = cap_data > pixel_count_thresh

        for b in range(100):
            start = buckets[b]
            end = buckets[b+1]
            ickfftb = (is_correct_key_frames >= start) & (is_correct_key_frames < end)
            ickfftb = np.array(is_correct_key_frames)[ickfftb]

            # TP = np.sum((cap_data_selected[ickfftb]))
            event_triggers = np.sum(cap_data_selected[start:end])
            events[i, j, b] = event_triggers
            # FN = np.sum(((~cap_data_selected)[ickfftb]))

            # print(TP, FP, FN)

            # if (TP==0) and (FP==0) and (FN==0):
            #     # precision_scores[i, j, b] = 0
            #     # recall_scores[i, j, b] = 0
            #     # f2_scores[i, j, b] = 0
            #     print("No events here")

            # precision = TP/(TP+FP)
            # recall = TP/(TP+FN)

            # # print(f"Bucket: {b}, Precision: {precision}, Recall: {recall}")

            # F2 = (1+2**2)*(precision*recall)/((2**2)*precision+recall)
            # precision_scores[i, j, b] = precision
            # recall_scores[i, j, b] = recall
            # f2_scores[i, j, b] = F2

            # print(f"Bucket {b}, F2 Score: {F2}")


# f1_scores = np.zeros((10, ))

import matplotlib.pyplot as plt

# print(81858 in is_correct_key_frames)
# print(precision_scores[2, 94])
# plt.figure(figsize=(10, 8))
threshold_data_cut = threshold_data[2, :81952]
print( events[2, 94, :])
# times = np.linspace(0, 81952, 100)
# for i, time in times:
#     threshold_data
plt.plot(np.arange(100), events[2, 94, :])
# plt.hist(threshold_data[2,:81952], np.linspace(0, 81592, 100))
# plt.imshow(recall_scores[2])
# plt.plot(buckets[:-1], recall_scores[2, 94])
# recall_scores[2, 94]
# plt.title('F2 score Heatmap')
# plt.xlabel('Pixel Count Threshold')
# plt.ylabel('Pixel Difference Threshold')
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.yticks(ticks=np.arange(len(pixel_value_thresholds)), labels=np.array(pixel_value_thresholds, dtype=np.int64))
# plt.show()

import scipy
import scipy.special
import numpy as np
import matplotlib.pyplot as plt

detection_times = []
with open(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\node\cam2_always_on.out", "r") as f:
    for line in f:
        line = line.strip().split(" ")
        if len(line) == 5:
            try:
                detection_times.append(float(line[-1][:-2])/1000)
            except:
                pass
detection_times = np.array(detection_times)

detection_times_tpu = []
with open(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\tpu.log", "r") as f:
    for line in f:
        line = line.strip().split(" ")
        if len(line) == 5:
            try:
                detection_times_tpu.append(float(line[-1][:-2])/1000)
            except:
                pass

print(len(detection_times_tpu)- len(detection_times))

plt.figure(figsize=(10, 8))
# data_min, data_max = min(min(detection_times), min(detection_times_tpu)), max(max(detection_times_tpu), max(detection_times))
plt.axvspan(1.2246, 1.2247, color='b', alpha=.5)
# plt.axvspan(0.88056, 0.88061, color='orange', alpha=.5)



plt.hist(detection_times, np.linspace(min(detection_times), max(detection_times), 100), label="float32 quantisation (CPU)")
# plt.hist(detection_times_tpu, np.linspace(data_min, data_max, 100), label="full integer quantisation (TPU)")
plt.legend()


plt.title('Time taken per frame for object detection \n (YOLOv10n float32)')
plt.xlabel('Time taken for object detection (s)')
plt.ylabel('Number of frames')
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
plt.show()
# print(np.mean(np.array(detection_times)))

# print(np.median(detection_times_tpu))

# detection_times_tpu
mean = np.mean(np.random.choice(detection_times_tpu, size=(len(detection_times_tpu), 1000)), axis=0)
print(np.median(mean))
scipy.stats.t.interval(0.95, len(mean)-1, loc=np.mean(mean), scale=scipy.stats.sem(mean))-np.mean(mean)

import scipy
def mean_confidence_interval(data, confidence=0.95):
    a = 1.0 * np.array(data)
    n = len(a)
    m, se = np.mean(a), scipy.stats.sem(a)
    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)
    return m, m-h, m+h


m, l, h = mean_confidence_interval(detection_times)

# h, l = scipy.stats.t.interval(0.95, len(detection_times)-1, loc=np.mean(detection_times), scale=scipy.stats.sem(detection_times))
h, l

"""# Buffer size"""

key_frames_raw = [71600,71668,72175,72196,73034,73093,73799,73825,75222,75490,80327,80351,80983,81040,82485,82518,83006,83097]

with open(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\cam1 copy.log", "r") as f:
    time_between_events = []
    time_during_events = []
    time_to_capture = []
    previous_event = 0
    current_event_start = 0
    current_frame = 0
    for line in f:
        line = line.strip().split(" ")
        # print(line)
        if line[-3] == "Current" and line[-2] == "frame:":
            current_frame = int(line[-1])
        elif line[-3] == "for" and line[-2] == "frame:":
            if current_frame == (previous_event+1):
                previous_event = current_frame
            else:
                print(f"from: {current_event_start}, to: {previous_event}, now: {current_frame}")
                time_during_events.append(previous_event-current_event_start+1)
                time_between_events.append(current_frame-previous_event)
                current_event_start = current_frame
                previous_event = current_frame

            if current_event_start == 0:
                current_event_start = current_frame
        elif line[-3] == "process" and line[-2] == "frame:":
            time_to_capture.append(float(line[-1][:-1]))

time_between_events = time_between_events[1:]
print(time_to_capture)

# event_triggered = np.load(r"data\triggered_events_for_may_01_prelim.npy")
# time_during_events = np.array([68, 21, 59, 26, 268, 24, 57, 33, 91, 14, 33, 85, 20, 97, 30, 91, 32, 80, 29, 37, 116, 160,
#                                25, 59, 58, 19, 60, 215, 17, 149, 28, 75, 15, 64, 28, 58, 18, 11]) #  22,   25,  660,   17,   20,   30,   26,   24,   15,   17,   22,   23,   30,
#    17,   28,   20,   25,   22,   12,  325,   22,   18,   30,   56,   22,   14,   25, 36,   27,   10,   11,   19,   33,   44,   26,   33, 2838,   31,
#    30,   19,   33, 19,   28,   25,   28,  646,   24,   26,  789, 4558,  530, 1095,  219,  523,  104,
#    70,   60, 1226,  237,  309,  623,  245, 1320,  752,  808, 223,  606,  452,  213, 527,  791,   44])
# print(t.interval(0.95, len(time_during_events)-1, loc=np.mean(time_during_events), scale=sem(time_during_events))-time_during_events.mean())
# print(np.median(time_during_events))

is_correct_key_frames = []
for i in range(0, 17, 2):
    start = key_frames_raw[i]
    stop = key_frames_raw[i+1]
    is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))
is_correct_key_frames = np.array(is_correct_key_frames)-71600

plt.figure(figsize=(10, 8))
plt.hist(time_during_events, 10)
plt.title('Histogram of the number of frames in each event')
plt.xlabel('Duration of events (frames)')
plt.ylabel('')
# plt.colorbar(label='F2 score')

# # plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# # plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
# plt.show()

# plt.figure(figsize=(10, 8))
# plt.hist(time_between_events, 10)
# plt.title('Histogram of the number of frames in between the start of each event')
# plt.xlabel('Duration (frames)')
# plt.ylabel('')
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
plt.show()

medians = []
for i in range(1000):
    medians.append(np.mean(np.random.choice(time_during_events, len(time_during_events))))
medians = np.array(medians)
print(t.interval(0.95, len(medians)-1, loc=np.mean(medians), scale=sem(medians))-medians.mean())
print(medians.mean())

medians = []
for i in range(1000):
    medians.append(np.mean(np.random.choice(time_between_events, len(time_between_events))))
medians = np.array(medians)
print(t.interval(0.95, len(medians)-1, loc=np.mean(medians), scale=sem(medians))-medians.mean())
print(medians.mean())

medians = []
for i in range(1000):
    medians.append(np.median(np.random.choice(time_to_capture, len(time_to_capture))))
medians = np.array(medians)
print(t.interval(0.95, len(medians)-1, loc=np.mean(medians), scale=sem(medians))-medians.mean())
print(medians.mean())

print(time_between_events)
print(time_during_events)

from scipy.stats import poisson
poisson.rvs(62.4)

import random

key_frames_raw = [71600,72175,73034,73799,75222,80327,80983,82485,83006]

# time_between_events = np.array([1624, 6528, 2158, 4415, 8424, 2170, 1228, 917, 230, 3362, 1308, 2715, 1595, 5842, 9487, 745, 569, 3467, 3360, 250,
#                                       707, 2433, 1379, 217, 884, 664, 1293, 667, ])
# time_between_entry_exit = np.array([470, 299, ])
# time_during_events = np.array([68, 21, 59, 26, 268, 24, 57, 33, 91, 14,   22,   25,  660,   17,   20,   30,   26,   24,   15,   17,   22,   23,   30,
#    17,   28,   20,   25,   22,   12,  325,   22,   18,   30,   56,   22,   14,   25, 36,   27,   10,   11,   19,   33,   44,   26,   33, 2838,   31,
#    30,   19,   33, 19,   28,   25,   28,  646,   24,   26,  789, 4558,  530, 1095,  219,  523,  104,
#    70,   60, 1226,  237,  309,  623,  245, 1320,  752,  808, 223,  606,  452,  213, 527,  791,   44])

num_of_simulations = 10
len_of_run = 10000
simulation = np.zeros((num_of_simulations, len_of_run))
for i in tqdm(range(num_of_simulations)):
    run = np.zeros(len_of_run)
    current_frame = 0
    while current_frame < len_of_run:
        # print(current_frame)
        entry = current_frame + np.random.choice(time_between_events)
        exit = entry + np.random.choice(time_during_events)
        if exit > len_of_run:
            exit = len_of_run
        run[entry:exit] = 1
        current_frame = exit
    simulation[i, :] = run

# detection_times
len_of_buffer = np.zeros((num_of_simulations, len_of_run))
for i in tqdm(range(num_of_simulations)):
    time_spent = 0
    time_to_detect = []
    for f in range(len_of_run):
        time_spent += random.choice(time_to_capture)
        if simulation[i, f] == 1:
            if time_to_detect == []:
                time_spent = 0
            time_to_detect.append(random.choice(detection_times))
        if time_to_detect != []:
            if time_to_detect[0] < time_spent:
                time_spent -= time_to_detect[0]
                time_to_detect.pop(0)

        len_of_buffer[i, f] = len(time_to_detect)

print(len_of_buffer)

max_len_buffer = np.max(len_of_buffer, axis=1)
print(t.interval(0.95, len(max_len_buffer)-1, loc=np.mean(max_len_buffer), scale=sem(max_len_buffer))-max_len_buffer.mean())
print(max_len_buffer.mean())

plt.figure(figsize=(10, 8))
for i in range(10):
    plt.plot(len_of_buffer[i], label=f"Simulation {i+1}")
plt.plot(np.ones(len_of_run)*200, label="Selected buffer size = 200")
plt.title('Simulations of Buffer Sizes')
plt.xlabel('Time (frames)')
plt.ylabel('Frames in buffer')
plt.legend()
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
plt.show()

with open(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\cam1.log", "r") as f:
    frames_in_queue = []
    current_frame = 0
    frame_trigger = []
    for line in f:
        line = line.strip().split(" ")
        # print(line)
        if line[-3] == "Current" and line[-2] == "frame:":
            current_frame = int(line[-1])
        if line[-2] == "queue:":
            frames_in_queue.append(int(line[-1]))
            frame_trigger.append(current_frame)
print(np.array(frame_trigger[:614])-71600)

plt.figure(figsize=(10, 8))
plt.plot(np.array(frame_trigger[:614])-71600, np.array(frames_in_queue[:614]))
plt.title('Number of frames in the buffer across time')
plt.xlabel('Annotated frame number')
plt.ylabel('Number of frames in buffer')
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
plt.show()

import random

distribution = np.array(detection_times)
cut_triggers = threshold_data[2] > 9400

num_of_simulations = 1
len_of_buffer = np.zeros((num_of_simulations, 81952))
for n in range(num_of_simulations):
    time_spent = 0
    time_to_detect = []
    for i in range(81952):
        if cut_triggers[i]:
            time_to_detect.append(random.choice(distribution))
        if cut_triggers[0] < time_spent:
            time_spent -= time_to_detect[0]
            time_to_detect.pop(0)
        if time_to_detect == []:
            time_spent = 0
        else:
            time_spent += 1/15
        len_of_buffer[n, i] = len(time_to_detect)

plt.figure(figsize=(10, 8))
plt.plot(len_of_buffer[0], label="Frames to be buffered")
plt.plot(np.ones(81952)*200, label="Selected buffer size = 200")
plt.title('Number of frames in the buffer across time')
plt.xlabel('Annotated frame number')
plt.ylabel('Number of frames in buffer')
plt.legend()
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
plt.show()

key_frames_raw = [190, 204, 3792, 3814, 4082, 4107, 5072, 5732, 6134, 6151, 6315, 6335, 6731, 6761, 6923, 6949, 7371, 7395, 8363, 8378, 8463, 8480, 8729, 8751, 8763, 8786, 9606, 9636, 11572, 11589, 12262, 12290, 16232, 16252,
                  16789, 16814, 19273, 19295, 20324, 20336, 24327, 24652, 26378, 26400, 27571, 27589, 28885, 28915, 29521, 29577, 32584, 32606, 33302, 33316, 34087, 34112, 34538, 34574, 35277, 35304, 35812, 35822, 37139, 37150,
                  37375, 37394, 37438, 37471, 38055, 38099, 38893, 38919, 39676, 39709, 42466, 45304, 51931, 51962, 52572, 52602, 55201, 55220, 55314, 55347, 56192, 56211, 56393, 56421, 58022, 58047, 58574, 58602, 59606, 60252,
                  63461, 63485, 63822, 63848, 64165, 64954, 64982, 69540, 69562, 70092, 70118, 71213, 71235, 71454, 71480, 72003, 72033, 72137, 72166, 72236, 72250, 72310, 72325, 73551, 73565, 73802, 74207, 74516, 74545, 75168,
                  75698, 75943, 76005, 77325, 77345, 78097, 78122, 78930, 78943, 79166, 79187, 79793, 79805, 80257, 80270, 80483, 80511, 81038, 81056, 81847, 81864, 81908, 81951]
starting_frames_raw = [3792, 5072, 6134, 6315, 7371, 8463, 8729, 11572, 16232, 19273, 24327, 26378, 27589, 28885, 32584, 34087, 35277, 37139, 37438, 38893, 42466, 44852, 51931, 55201, 55314, 58022, 59606, 63822, 64165, 70118,
                       71458, 72033, 72166, 74516,  75943, 78097, 79166, 80257, 80511, 81056, 81908]
starting_frames = np.array(key_frames_raw)[np.arange(len(key_frames_raw))%2 == 0][:-1]
stopping_frames = np.array(key_frames_raw)[np.arange(len(key_frames_raw))%2 == 1]
event_duration = stopping_frames-starting_frames
print(starting_frames, event_duration)
# is_correct_key_frames = []
# for i in range(0, 182, 2):
#     start = key_frames_raw[i]
#     stop = key_frames_raw[i+1]
#     is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))
# print(is_correct_key_frames[-1])

time_between_event_trigggers = []
t = 0
for f in starting_frames_raw:
    time_between_event_trigggers.append(f-t)
    t = f
plt.hist(time_between_event_trigggers, 10)

plt.hist(event_duration, 20)

from tqdm import tqdm

# cap = cv2.VideoCapture(r"C:\Users\tanji\Desktop\recordings\evaluation\2025-05-01_12-37-19.mp4_annotated.mp4")
contour_area = np.linspace(0, 9000, 10)
pixel_count_thresholds = np.linspace(0, 10000, 101)


# event_triggered = np.zeros((10, 11767))
event_triggered = np.zeros((10, 11767))


cap = cv2.VideoCapture(r"C:\Users\tanji\Desktop\recordings\evaluation\7_annotated.mp4")
old_frame = None
frame_number = 71600
while True:
    print(frame_number)
    ret, frame = cap.read()

    if not ret:
        break

    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    frame = cv2.GaussianBlur(frame, (21, 21), 0)

    if old_frame is None:
        old_frame = frame

    diff = cv2.absdiff(old_frame, frame)
    _, thresh = cv2.threshold(diff, 20, 255, cv2.THRESH_BINARY)
    thresh = cv2.dilate(thresh, None, iterations=2)

    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # big_contours = []
    for i, ca in enumerate(contour_area):
        for c in contours:
            if cv2.contourArea(c) > ca:  # <-- Tune this value
                # big_contours.append(c)
                event_triggered[i, frame_number-71600] = 1
                break
    # img = cv2.drawContours(frame, big_contours, -1, (0,255,75), 2)
    # cv2.imshow("frame", img)
    # if cv2.waitKey(1) == ord("q"):
    #     break
    # for i, pct in enumerate(pixel_count_thresholds):
    #     if np.sum(thresh/255) > pct:
    #         event_triggered[i, frame_number-71600] = 1

    frame_number += 1

    old_frame = frame

cv2.destroyAllWindows()
cap.release()

# np.save(r"data\triggered_events_for_may_01_prelim.npy", event_triggered)

from sklearn.metrics import confusion_matrix

key_frames_raw = [71600,71668,72175,72196,73034,73093,73799,73825,75222,75490,80327,80351,80983,81040,82485,82518,83006,83097]
# event_triggered = np.load(r"data\triggered_events_for_may_01_prelim.npy")

is_correct_key_frames = []
for i in range(0, 17, 2):
    start = key_frames_raw[i]
    stop = key_frames_raw[i+1]
    is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))
is_correct_key_frames = np.array(is_correct_key_frames)-71600

ground_truth_events = np.zeros(11766)
ground_truth_events[is_correct_key_frames] = 1

tn_fp_fn_tp = np.zeros((4, 10))
precision_scores = np.zeros(10)
recall_scores = np.zeros(10)
f2_scores = np.zeros(10)


for i in range(10):
    tn, fp, fn, tp = confusion_matrix(ground_truth_events, event_triggered[i, 1:]).ravel()
    tn_fp_fn_tp[:, i] = [tn, fp, fn, tp]
    print(tn, fp, fn, tp)

    precision = tp/(tp+fp)
    recall = tp/(tp+fn)
    f2 = 5/((4/precision)+(1/recall))

    # precision, recall, f2, _ = precision_recall_fscore_support(ground_truth_events, event_triggered[i,:], beta=2)
    # print(precision)
    precision_scores[i] = precision
    recall_scores[i] = recall
    f2_scores[i] = f2

# higher_events_triggered = np.load(r"data\triggered_events_for_may_01_prelim.npy")
# pr_graph_total = np.vstack([pr_graph_low,pr_graph])
# print(pr_graph_total)

# pr_abs_value = np.column_stack([recall_scores, precision_scores])
# print(pr_abs_value)
# pr_abs_value = pr_abs_value[pr_abs_value[:, 0].argsort()]
# print(pr_abs_value)

# pr_low = np.column_stack([recall_scores, precision_scores])
# print(pr_low)
# pr_low = pr_low[pr_low[:, 0].argsort()]
# print(pr_low)

# pr_high = np.column_stack([recall_scores, precision_scores])
# print(pr_high)
# pr_high = pr_high[pr_high[:, 0].argsort()]
# print(pr_high)

# pr_con = np.vstack([pr_low, pr_high])
# print(pr_con)
# pr_con = pr_con[pr_con[:, 0].argsort()]
# print(pr_con)

# print(precision_scores)
# print(recall_scores)
# print(f2_scores)
# print(tn_fp_fn_tp_copy[10:])
# pr_graph = zip(recall_scores, precision_scores)
# pr_graph_total = np.array(np.sort(pr_graph_total, axis=0))
# print(pr_graph_total)

plt.figure(figsize=(10, 8))
# plt.plot(np.linspace(1000, 9000, 10), f2_scores, "o-")
plt.title('Precision-Recall Curves for different models of determining events')
plt.plot(pr_con[3:,0], pr_con[3:,1], label="Contour area")
plt.plot(pr_abs_value[:,0], pr_abs_value[:, 1], label="Absolute difference")
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
plt.show()

# f2_abs = f2_scores.copy()
# # print(pr_graph_total)
print(precision_scores)
print(recall_scores)
print(f2_abs)
# print(contour_area)

plt.figure(figsize=(10, 8))
# plt.plot(np.linspace(1000, 9000, 10), f2_scores, "o-")
plt.title('F2 scores of different models')
plt.xlabel('Contour Area Threshold/Pixel Count Threshold (pixels)')
plt.errorbar(np.linspace(1000, 9000, 10), f2_scores, yerr=[0.0005, 0.0008, 0.0009, 0.001, 0.001,0.001, 0.001, 0.001, 0.001, 0.001], fmt="o-", label="Contour area")
plt.plot(np.linspace(0, 10000, 101), f2_abs, label="Absolute difference")

plt.ylabel('F2 Score')
plt.legend()
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.xticks(ticks=np.linspace(1, 3, 10), labels=np.linspace(0, 3, 10))
plt.show()

"""tn   fp   fn tp
8646 2464 72 584
10262 848 277 379
10444 666 317 339
10612 498 344 312
10792 318 369 287
10890 220 397 259
10943 167 410 246
10979 131 422 234
10990 120 433 223
10996 114 440 216

# Bootstrapping Confusion matrix
"""

from scipy.stats import binom, t, sem
from sklearn import metrics
import math

total = 164160
TP = 75
FP = 33
FN = 8

probability_of_event = (TP+FN)/total
probability_of_det = TP/(TP+FN)
probability_of_det_non_event = FP/(total-TP-FN)

events = binom.rvs(total, probability_of_event, size=1000)
TP = binom.rvs(events, probability_of_det)
FN = events-TP
FP = binom.rvs((total-events), probability_of_det_non_event)
TN = total - events - FP

accuracy = TP/(TP+FN+FP)
precision = TP/(TP+FP)
recall = TP/(TP+FN)
F1 = 2*(precision*recall)/(precision+recall)
F2 = 5/((4/precision)+(1/recall))
MCC = (TP*TN - FP*FN) / (((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**(0.5))

print(t.interval(0.95, len(accuracy)-1, loc=np.mean(accuracy), scale=sem(accuracy))-accuracy.mean())
print(t.interval(0.95, len(precision)-1, loc=np.mean(precision), scale=sem(precision))-precision.mean())
print(t.interval(0.95, len(recall)-1, loc=np.mean(recall), scale=sem(recall))-recall.mean())
print(t.interval(0.95, len(F1)-1, loc=np.mean(F1), scale=sem(F1))-F1.mean())
# print(t.interval(0.95, len(F2)-1, loc=np.mean(F2), scale=sem(F2))-F2.mean())
print(t.interval(0.95, len(MCC)-1, loc=np.mean(MCC), scale=sem(MCC))-MCC.mean())

from scipy.stats import uniform

time_error = 0.0001
energy_error = 0.5

time_measurement = [4.4294, 9.4908]
energy_measurement = [22912]

uniform.rvs(size=1000)

"""# Weighted Pixel Activations"""

import numpy as np
import cv2

key_frames_raw = np.load(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\key_frames_raw_02_26.npy")
is_correct_key_frames = []
for i in range(0, 182, 2):
    start = key_frames_raw[i]
    stop = key_frames_raw[i+1]
    is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))

num_of_pixels_changed = []
cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\2025-02-26_14-38-06.mp4")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

pixel_activation_count = np.zeros((height, width))

prev_frame_grey = None
raw_frame_num = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    if prev_frame_grey is None:
        prev_frame_grey = frame_grey

    frame_difference = cv2.absdiff(prev_frame_grey, frame_grey)

    prev_frame_grey = frame_grey

    if raw_frame_num in is_correct_key_frames:
        _, filter_frame = cv2.threshold(frame_difference, 20, 1, cv2.THRESH_BINARY)
        pixel_activation_count += filter_frame

    if raw_frame_num == key_frames_raw[-1]:
        break

    raw_frame_num += 1

cap.release()

pixel_activation_count_normalised = pixel_activation_count/np.sum(pixel_activation_count)
# np.save(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\pixel_activation_map.npy", pixel_activation_count_normalised)

import matplotlib.pyplot as plt

plt.imshow(cv2.GaussianBlur(pixel_activation_count, (19,19), 0), cmap='viridis', aspect="auto")
# plt.imshow(pixel_activation_count, cmap='viridis', aspect="auto")
plt.title('Pixel Activations Heatmap')
# plt.xlabel('Pixel Count Threshold')
# plt.ylabel('Pixel Difference Threshold')
# plt.axis("off")
plt.colorbar(label='Number of Activations')
# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
# plt.yticks(ticks=np.arange(len(pixel_value_thresholds)), labels=np.array(pixel_value_thresholds, dtype=np.int64))
plt.show()

import numpy as np
import cv2

num_of_pixels_changed = []
cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\2025-02-26_14-38-06.mp4")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

pixel_count_thresholds = np.linspace(0, 1, 101)
precision_scores = np.zeros(101)
recall_scores = np.zeros(101)
f2_score_scores = np.zeros(101)

for i, threshold in enumerate(pixel_count_thresholds):
    prev_frame_grey = None
    raw_frame_num = 0
    TP = 0
    TN = 0
    FP = 0
    FN = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        if prev_frame_grey is None:
            prev_frame_grey = frame_grey

        frame_difference = cv2.absdiff(prev_frame_grey, frame_grey)

        prev_frame_grey = frame_grey
        _, filter_frame = cv2.threshold(frame_difference, 20, 1, cv2.THRESH_BINARY)

        pixels_changed = np.sum(filter_frame*pixel_activation_count_normalised)
        if pixels_changed > threshold:
            if raw_frame_num in is_correct_key_frames:
                TP += 1
            else:
                FP += 1
        else:
            if raw_frame_num in is_correct_key_frames:
                FN += 1
            else:
                TN += 1

        if raw_frame_num == key_frames_raw[-1]:
            break

        raw_frame_num += 1

    precision = TP/(TP+FP)
    recall = TP/(TP+FN)

    F2 = (1+2**2)*(precision*recall)/((2**2)*precision+recall)

    print(f"Precision: {precision}, Recall: {recall}, F2: {F2}")


    precision_scores[i] = precision
    recall_scores[i] = recall
    f2_scores[i] = F2

cap.release()

"""# Machine Learning Classifier"""

import torch.nn as nn

"""# Wide Angle Lens eval"""

import cv2
import numpy as np
import yaml
# from ultralytics import YOLO

with open(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\node\config\calibration.yaml", "r") as f:
    config = yaml.safe_load(f)
D = np.array(config["D"])
K = np.array(config["K"])

scaled_K = K
scaled_K[2][2] = 1.0

K2 = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(scaled_K, D, (1280, 720), np.eye(3), balance=1)
map1, map2 = cv2.fisheye.initUndistortRectifyMap(scaled_K, D, np.eye(3), K2, (1280, 720), cv2.CV_16SC2)

raw_cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\2025-04-18_19-17-25.mp4")

width = int(raw_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(raw_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

key_frame_num = np.random.random_integers(0,100, None)

frame_num = 0
while raw_cap.isOpened():
    ret, raw_frame =  raw_cap.read()

    if not ret:
        break

    frame = cv2.remap(raw_frame, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)


    # cv2.imshow("frame", frame)

    if frame_num == key_frame_num:
        cv2.imwrite("Bike shed random frame.png", raw_frame)
        break
    else:
        frame_num += 1

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cv2.destroyAllWindows()
raw_cap.release()

import yaml

raw = cv2.imread("Bike shed random frame.png")
with open(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\node\config\calibration.yaml", "r") as f:
    config = yaml.safe_load(f)
D = np.array(config["D"])
K = np.array(config["K"])

scaled_K = K
scaled_K[2][2] = 1.0

K2 = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(scaled_K, D, (1280, 720), np.eye(3), balance=1)
map1, map2 = cv2.fisheye.initUndistortRectifyMap(scaled_K, D, np.eye(3), K2, (1280, 720), cv2.CV_16SC2)
frame = cv2.remap(raw, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)

cv2.imwrite("Bike shed random frame.png", frame)

from ultralytics import YOLO

frame = cv2.imread("Bike shed random frame.png")

model = YOLO(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\node\src\models\yolov10n_saved_model\yolov10n_float32.tflite", task="detect")
result = model.predict(frame)[0].plot()
cv2.imwrite(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\Dissertation\Dissertation\first draft\roi_example_random_frame.png", result)

def mouse_callback(event,x,y,flags,param):
    global ix,iy,drawing, mask, mask_uncomitted, color, initial_img, bike_lot_pts, is_occupied

    if event == cv2.EVENT_LBUTTONDOWN:
        print(x, y)

cap = cv2.VideoCapture(r"C:\Users\tanji\Desktop\recordings\evaluation\events.mp4")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

ret, frame = cap.read()
cv2.imshow("frame", frame)
cv2.setMouseCallback('frame', mouse_callback)


while cap.isOpened():
    cv2.imshow("frame", frame)

    if cv2.waitKey(1) == ord("q"):
        break

cv2.destroyAllWindows()
cap.release()

import yaml

source_points = np.array([[272, 530], [301, 296], [389, 303], [448, 458]], dtype=np.float64)
destination_points = np.array([[350, 0], [940, 0], [940, 290], [450, 290]], dtype=np.float64)

H, status = cv2.findHomography(source_points, destination_points)
with open("H.yaml", "w") as f:
    yaml.dump({"H1": H.tolist()}, f)

with open(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\cam2.log", "r") as f:
    bikes = []
    for line in f:
        line = line.strip().split()
        if line[-2]=="bikes:":
            bikes.append(int(line[-1]))

plt.figure(figsize=(10, 8))
# plt.plot(np.linspace(1000, 9000, 10), f2_scores, "o-")
plt.title('Number of detected bikes by YOLO compared to the true value')
plt.xlabel('Frame number')
plt.plot(np.ones(len(bikes))*9, label="True number of bikes")
plt.plot(bikes, label="Detected bikes")

plt.ylabel('Number of bikes')
plt.legend()
# plt.colorbar(label='F2 score')

# plt.xticks(ticks=np.linspace(0, 100, 11), labels=np.linspace(0, 10000, 11, dtype=np.int64))
plt.yticks(ticks=np.linspace(6, 10, 5, dtype=np.int8))
plt.show()

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(10, 8))
# plt.plot(np.linspace(1000, 9000, 10), f2_scores, "o-")
plt.title("The number of bikes for each gap observed between true location and assigned lot")
plt.xlabel('Number of lots between true lot and assigned lot')
# plt.bar([0,1,2,3], [6, 5, 3, 3])

plt.ylabel('Number of bikes')
# plt.legend()
# plt.colorbar(label='F2 score')
plt.hlines([0,1,2,3], xmin=0, xmax=[6, 5, 3, 3])

plt.xticks(ticks=np.linspace(0, 3, 4, dtype=np.int32))
# plt.yticks(ticks=np.linspace(6, 10, 5, dtype=np.int8))
plt.show()

"""# Saving photos"""

raw_cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\Untitled video (1).mp4")

width = int(raw_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(raw_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

while raw_cap.isOpened():
    ret, raw_frame =  raw_cap.read()

    if not ret:
        break

    cv2.imwrite(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\Dissertation\Dissertation\first draft\loi_example.png", raw_frame)

    break
    cv2.imshow("frame", frame)


    # cv2.imwrite("Bike shed undistorted.png", raw_frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cv2.destroyAllWindows()
raw_cap.release()

"""# Online analysis"""

import numpy as np
import cv2
from ultralytics import YOLO

# model = YOLO()

key_frames_raw = np.load(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\key_frames_raw_02_26.npy")
is_correct_key_frames = []
for i in range(0, 182, 2):
    start = key_frames_raw[i]
    stop = key_frames_raw[i+1]
    is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))

num_of_pixels_changed = []
cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_raw.mp4")
# cap = cv2.VideoCapture(r"C:\Users\tanji\Desktop\recordings\2025-04-30_13-20-44.mp4")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

pixel_activation_count = np.zeros((height, width))

prev_frame_grey = None
raw_frame_num = 0
next_frame = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # if raw_frame_num != next_frame:
    #     raw_frame_num += 1
    #     continue

    # if raw_frame_num in is_correct_key_frames:
    #     cv2.imwrite(fr"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\04_30\{raw_frame_num}_numbered.png", frame)

    cv2.imwrite(fr"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\04_30\{raw_frame_num}_numbered.png", frame)


    if raw_frame_num == key_frames_raw[-1]:
        break

    raw_frame_num += 1
    next_frame += 15

cap.release()

import numpy as np
import cv2
from ultralytics import YOLO

# model = YOLO()

key_frames_raw = np.load(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\key_frames_raw_02_26.npy")
is_correct_key_frames = []
for i in range(0, 182, 2):
    start = key_frames_raw[i]
    stop = key_frames_raw[i+1]
    is_correct_key_frames += list(np.linspace(start, stop, stop-start+1, dtype=np.int64))

num_of_pixels_changed = []
# cap = cv2.VideoCapture(r"C:\Users\tanji\OneDrive\Cambridge\2\Project\recordings\Detections\02-26_raw.mp4")
cap = cv2.VideoCapture(r"C:\Users\tanji\Desktop\recordings\ok recordings\2025-04-30_13-08-54.mp4")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

pixel_activation_count = np.zeros((height, width))

prev_frame_grey = None
raw_frame_num = 0
next_frame = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # if raw_frame_num != next_frame:
    #     raw_frame_num += 1
    #     continue

    # if raw_frame_num in is_correct_key_frames:
    #     cv2.imwrite(fr"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\04_30\{raw_frame_num}_numbered.png", frame)
    cv2.putText(frame, f"{raw_frame_num}", (1150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 3)

    # cv2.imshow("frame", frame)
    if raw_frame_num == 2710:
        cv2.imwrite(fr"C:\Users\tanji\OneDrive\Cambridge\2\Project\ShedSense\evaluation\data\04_30\{raw_frame_num}_numbered.png", frame)


    if raw_frame_num == key_frames_raw[-1]:
        break

    raw_frame_num += 1
    next_frame += 15

    if cv2.waitKey(1) == ord("q"):
        break

cap.release()

import

def inside_rect_index(x, y, pts):
    for i, [x1,x2,y1,y2, _] in enumerate(pts):
        if (x1 < x) and (x < x2):
            if (y1 < y) and (y < y2):
                return i
    return None

# mouse callback function
def mouse_callback(event,x,y,flags,param):
    global ix,iy,drawing, mask, mask_uncomitted, color, initial_img, bike_lot_pts, is_occupied

    if event == cv2.EVENT_LBUTTONDOWN:
        drawing = True
        ix,iy = x,y
        mask_uncomitted = np.copy(mask)

        if is_occupied:
            color = (0, 0, 255)
        else:
            color = (0, 255, 0)

    elif event == cv2.EVENT_MOUSEMOVE:
        if drawing:
            cv2.rectangle(mask_uncomitted,(ix,iy),(x,y),color,-1)

    elif event == cv2.EVENT_LBUTTONUP:
        if drawing:
            drawing = False
            if (abs(x-ix) > 1) and (abs(y-iy) > 1):
                if ix > x:
                    ix, x = x, ix
                if iy > y:
                    iy, y = y, iy

                cv2.rectangle(mask,(ix,iy),(x,y),color,-1)
                bike_lot_pts.append([ix,x,iy,y, color])
        # cv.rectangle(img_copy,(ix,iy),(x,y),(0,255,0),-1)

    elif event == cv2.EVENT_RBUTTONDOWN:
        drawing = False

    elif event == cv2.EVENT_LBUTTONDBLCLK:
        index = inside_rect_index(x, y, bike_lot_pts)
        if index is not None:
            bike_lot_pts.pop(index)
            # print(bike_lot_pts)

            mask = np.zeros_like(initial_img)
            for x1,x2,y1,y2, c in bike_lot_pts:
                cv2.rectangle(mask, (x1, y1), (x2, y2), c, -1)

def start_lot_drawing(img):
    print("Starting lot drawing")
    global ix,iy,drawing, mask, mask_uncomitted, bike_lot_pts, is_occupied, initial_img

    drawing = False
    is_occupied = False
    ix,iy = -1,-1
    initial_img = np.copy(img)

    mask = np.zeros_like(initial_img, dtype=np.uint8)
    bike_lot_pts = []

    cv2.namedWindow('Initialise bike lots')
    cv2.setMouseCallback('Initialise bike lots', mouse_callback)

    while True:
        out = np.copy(initial_img)
        if drawing:
            out_mask = mask_uncomitted.astype(bool)
            out[out_mask] = cv2.addWeighted(initial_img, 0.5, mask_uncomitted, 0.5, 0)[out_mask]
        else:
            out_mask = mask.astype(bool)
            out[out_mask] = cv2.addWeighted(initial_img, 0.5, mask, 0.5, 0)[out_mask]

        cv2.imshow('Initialise bike lots',out)

        key = cv2.waitKey(1)
        if key == ord("m"):
            is_occupied = not is_occupied
        elif key == ord('q'):
            break
        elif key == ord("s"):
            cv2.imwrite("drawn frame.png", out)
            cv2.destroyAllWindows()
            return bike_lot_pts

start_lot_drawing(cv2.imread(r"C:\Users\tanji\OneDrive\Pictures\vlcsnap-2025-05-09-22h14m07s591.png"))

"""# Confusion Matrix"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np
import seaborn as sns

people_entry_true = np.concatenate([np.ones(84), np.ones(3), np.zeros(21), np.zeros(165044)])
people_entry_det = np.concatenate([np.ones(84), np.zeros(3), np.ones(21), np.zeros(165044)])

cm = confusion_matrix(people_entry_true, people_entry_det)

# log_cm = np.log1p(cm)
# sns.heatmap(log_cm, annot=True, fmt=".2f", cmap='Blues')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Entry", "Entry"])

disp.plot(colorbar=False)
# plt.rcParams.update({"font.size": 16})
plt.show()

people_entry_true = np.concatenate([np.ones(87), np.ones(1), np.zeros(37), np.zeros(165027)])
people_entry_det = np.concatenate([np.ones(87), np.zeros(1), np.ones(37), np.zeros(165027)])

cm = confusion_matrix(people_entry_true, people_entry_det)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Exit", "Exit"])

disp.plot(colorbar=False)
plt.show()

people_entry_true = np.concatenate([np.ones(171), np.ones(4), np.zeros(58), np.zeros(163927)])
people_entry_det = np.concatenate([np.ones(171), np.zeros(4), np.ones(58), np.zeros(163927)])

cm = confusion_matrix(people_entry_true, people_entry_det)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Event", "Event"])

disp.plot(colorbar=False)
plt.show()

people_entry_true = np.concatenate([np.ones(19), np.ones(2), np.zeros(8), np.zeros(165123)])
people_entry_det = np.concatenate([np.ones(19), np.zeros(2), np.ones(8), np.zeros(165123)])

cm = confusion_matrix(people_entry_true, people_entry_det)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Entry", "Entry"])

disp.plot(colorbar=False)
plt.show()

people_entry_true = np.concatenate([np.ones(56), np.ones(6), np.zeros(25), np.zeros(165065)])
people_entry_det = np.concatenate([np.ones(56), np.zeros(6), np.ones(25), np.zeros(165065)])

cm = confusion_matrix(people_entry_true, people_entry_det)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Exit", "Exit"])

disp.plot(colorbar=False)
plt.show()

people_entry_true = np.concatenate([np.ones(75), np.ones(8), np.zeros(33), np.zeros(164044)])
people_entry_det = np.concatenate([np.ones(75), np.zeros(8), np.ones(33), np.zeros(164044)])

cm = confusion_matrix(people_entry_true, people_entry_det)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Event", "Event"])

disp.plot(colorbar=False)
plt.show()